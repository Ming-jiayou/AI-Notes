多智能体大语言模型系统为何会失败？

摘要

尽管对多智能体大语言模型系统（MAS）的兴趣日益增长，但与单智能体框架相比，其在主流基准测试中的性能提升往往十分有限。这一差距凸显了系统性分析阻碍MAS有效性的挑战的必要性。

我们提出了MAST（多智能体系统故障分类法），这是首个基于实证研究的分类体系，旨在理解多智能体系统（MAS）的故障。我们对七个流行MAS框架在200多个任务上的表现进行了分析，涉及六位专家人工标注者。通过这一过程，我们识别出14种独特的故障模式，并将其归纳为三个核心类别：(i) 规范性问题，(ii) 智能体间不一致，以及(iii) 任务验证问题。MAST是通过严格的标注者间一致性研究逐步形成的，达到了0.88的Cohen’s Kappa评分。为支持可扩展的评估，我们开发了一套与MAST集成的经过验证的“大语言模型作为裁判”（LLM-as-a-Judge）评估流程。我们通过两个案例研究展示了MAST在分析故障和指导MAS开发方面的实际应用价值。研究发现表明，这些识别出的故障需要更复杂的解决方案，为未来的研究指明了清晰的路径。我们已开源完整的数据集和LLM标注工具，以推动MAS的持续发展。

1、引言  

近年来，基于大语言模型（LLM）的智能体系统在人工智能社区中引起了广泛关注（Patil 等，2023；Packer 等，2024；Wang 等，2024a）。这种日益增长的兴趣源于智能体系统能够处理复杂、多步骤的任务，并动态地与各种环境进行交互，使基于LLM的智能体系统非常适合解决现实世界中的问题（Li 等，2023）。基于这一特性，多智能体系统在多个领域中得到了日益深入的研究，例如软件工程（Qian 等，2023；Wang 等，2024d）、药物发现（Gottweis 等，2025；Swanson 等，2024）、科学模拟（Park 等，2023b），以及通用智能体（Liang 等，2025；Fourney 等，2024）。

尽管关于代理的正式定义仍存在争议（Cheng等, 2024；Xi等, 2023；Guo等, 2024a；Li等, 2024b；Wang等, 2024b），但在本研究中，我们将基于大语言模型的代理定义为一种具有提示规范（初始状态）、对话轨迹（状态）以及能够与环境进行交互如工具（行动）使用的人工实体。随后，多智能体系统（MAS）被定义为一组通过协同机制相互作用的智能体，从而实现集体智能。MAS被设计用于协调各方努力，支持任务分解、性能并行化、上下文隔离、专用模型集成以及多样化的推理讨论（He 等, 2024b；Mandi 等, 2023；Zhang 等, 2024；Du 等, 2023；Park 等, 2023a；Guo 等, 2024a）。

尽管多智能体系统（MAS）的采用日益广泛，但其性能提升往往相较于单智能体框架（Xia et al., 2024）或简单的基线方法（如 best-of-N 采样，Kapoor et al., 2024）仍十分有限。我们的实证分析发现，即使是最先进的开源多智能体系统也存在极高的失败率；例如，ChatDev（Qian et al., 2023）在我们的 ProgramDev 基准测试中仅达到了 33.33% 的正确率（见图1）。此外，目前尚无共识明确如何构建鲁棒且可靠的多智能体系统。这促使我们提出一个根本性的问题：多智能体系统为何会失败？

为了理解多智能体系统（MAS）的失败情况，我们首次采用扎根理论（Glaser & Strauss, 1967）对MAS的执行轨迹进行了系统性评估。我们分析了200条来自多样化任务的对话轨迹（每条轨迹平均超过15,000行文本），涵盖了7个流行的开源MAS框架，并由六位专家人工标注员参与。我们将失败定义为MAS未能实现预期任务目标的情况。为确保标注一致性，三位标注员独立标注了15条轨迹，取得了较高的标注者间一致性（Cohen's Kappa = 0.88）。通过这一全面分析，我们识别出14种不同的失败模式，并将其归类为3个类别。为此，我们提出了多智能体系统失败分类法（MAST），这是首个针对MAS的结构化失败分类体系，如图2所示。构建该分类体系是一个非平凡的过程，需要严谨的分析以明确、可泛化的失败边界。我们并不认为MAST涵盖了所有可能的失败模式；相反，它作为迈向统一理解MAS失败现象的首个基础性步骤。

为实现可扩展的自动化评估，我们引入了一种基于OpenAI o1模型的“大语言模型作为评判者”（LLM-as-a-judge）评估流水线（Zheng et al., 2023）。该流水线经过专家标注的验证，取得了0.77的Cohen’s Kappa一致率，表明其可靠性。为进一步评估MAST的泛化能力，我们将该流水线应用于初始开发MAST时未涉及的两个多智能体系统（MAS）——Magnetic-One（Fourney et al., 2024）和OpenManus（Liang et al., 2025），以及两个基准测试集——GAIA（Mialon et al., 2023）和MMLU（Hendrycks et al., 2020）。在未见过的领域和基准测试中，我们实现了高达0.79的Cohen’s Kappa一致性，充分展示了MAST的广泛适用性。

为展示MAST在指导MAS开发中的实际应用价值，我们通过案例研究分析了改进角色定义和架构变更等干预措施。通过我们的LLM标注器，我们获取了干预前后的详细失败分解结果，展示了MAST如何为调试与开发提供可操作的洞察。尽管干预带来一定改善（如ChatDev性能提升15.6%），但结果表明，简单的修复措施仍不足以实现可靠的整体MAS性能。要真正缓解识别出的失败问题，必须在系统设计层面进行更根本的变革。

这些发现表明，MAST反映的是当前多智能体系统固有的设计挑战，而不仅仅是特定实现中的偶然现象。通过系统化地定义失败类型，MAST不仅为失败诊断提供了框架，也向学术社区提出了具体的研究方向。为此，我们已将全部追踪记录、标注数据和LLM标注流水线开源，以推动更稳健、更可靠的MAS研究。

虽然有人可能简单地将这些失败归因于当前大语言模型的局限性（如幻觉、对齐偏差），但我们推测，仅靠基础模型能力的提升，远远不足以解决MAST所涵盖的全部问题。相反，我们认为优秀的MAS设计必须包含组织层面的理解——即便是由高能力个体组成的组织，若其组织结构存在缺陷，仍可能遭遇灾难性失败（Perrow, 1984）。以往关于高可靠性组织的研究也表明，明确的设计原则可有效预防此类失败（Roberts, 1989；Rochlin, 1996）。我们的研究发现与这些理论一致：许多MAS失败的根源在于组织设计和智能体协调方面的挑战，而非单一智能体能力的限制。

本文的主要贡献如下：

• 我们提出了MAST，这是首个基于实证研究的多智能体系统失败分类体系，为失败的定义与理解提供了结构化框架。  
• 我们开发了一套可扩展的“大语言模型作为评判者”评估流水线，并将其与MAST深度融合，用于分析MAS性能、诊断失败模式并理解失败构成。  
• 通过案例研究，我们证明MAST识别出的失败多源于系统设计问题，而不仅仅是LLM本身的局限或简单提示遵循，因此需要超越表面修补的结构性重新设计。  
• 我们完整开源了相关数据与代码，包括200余条对话追踪记录、LLM评估流水线、标注结果以及详尽的专家标注，以促进后续研究发展。

2、相关工作 

2.1. 代理系统中的挑战  

代理系统所展现出的潜力激发了研究者们对解决特定挑战的探索。例如，Agent Workflow Memory（Wang等，2024e）通过引入工作流记忆机制，应对长周期网络导航问题；DSPy（Khattab等，2023）致力于解决编程型代理流程中的问题；而StateFlow（Wu等，2024b）则聚焦于代理工作流中的状态控制，以提升任务求解能力。此外，多项综述也特别指出了多代理系统（MAS）中面临的挑战与潜在风险（Han等，2024；Hammond等，2025）。尽管这些研究在理解特定问题或提供高层次概览方面具有重要意义，但它们并未提供一个细致入微、基于实证的分类体系，以阐明MAS在多种系统和任务中失败的根本原因。目前也已存在大量评估代理系统的基准测试（Jimenez等，2024；Peng等，2024；Wang等，2024c；Anne等，2024；Bettini等，2024；Long等，2024）。这些评估至关重要，但其主要采用自上而下的视角，侧重于总体性能或高层次目标，如可信性与安全性（Liu等，2023c；Yao等，2024b）。我们的研究通过提供一种自下而上的分析方法，聚焦于识别MAS中的具体失败模式，从而补充了上述工作。

2.2 代理系统的设计原则  

多项研究指出了构建稳健代理系统所面临的挑战，并提出了设计原则，这些原则通常聚焦于单代理场景。例如，Anthropic 的博客文章强调了模块化组件的重要性，并建议避免过于复杂的框架（Anthropic, 2024a）。类似地，Kapoor 等人（2024）展示了复杂性如何阻碍实际应用。本文将这些洞见拓展至多代理场景，系统性地探究了失败模式。我们提出了一个分类体系（MAST），为理解多代理系统为何失败提供了结构化视角，从而引导未来研究朝向更具鲁棒性的系统设计方向发展，契合了当前对更清晰规范和设计原则的呼吁（Stoica 等，2024a）。

2.3 LLM系统中的失败分类  

尽管对大型语言模型（LLM）代理的兴趣日益增长，但专门系统性地描述其失败模式的研究仍较为有限，尤其是在多智能体系统（MAS）领域。尽管Bansal等（2024）梳理了人机交互中的挑战，但我们的研究重点聚焦于自主多智能体系统执行过程中的失败现象。其他相关工作包括针对多轮LLM对话的评估分类（Bai等，2024），或特定能力如代码生成的分类（Da等，2023）。这些研究与我们旨在构建可推广的多智能体系统失败分类体系的目标存在显著差异。
  
进一步的相关研究致力于通过不同方法改进多智能体系统：AgentEval（Arabzadeh 等，2024）提出一种框架，利用大语言模型智能体来定义和量化多维评估标准，以反映任务对最终用户的实用性；而 AGDebugger（Epperson 等，2025）则引入了一个交互式工具，使开发者能够通过检查和编辑消息历史来调试和引导智能体团队。

因此，据我们所知，MAST 是首个基于实证研究、全面且专门针对多智能体系统失败的分类体系。识别这些模式凸显了针对多智能体系统独特挑战，持续推进稳健评估指标及缓解策略研究的必要性。

3、研究方法  

本节描述了我们识别多智能体系统（MAS）中主导失效模式并建立失效模式结构化分类体系的方法。图3概述了该工作流程。

首先，我们需要指出，收集并提出故障模式的分类体系是一项非常复杂的任务，需要投入大量精力和深思熟虑：该分类体系应具有足够的广度，以涵盖不同类型的故障模式，这些模式可能出现在各种多智能体系统（MAS）及基准测试中；同时，也应具备足够的精确性和细致性，以便对观察到的故障提供深入的洞察。此外，当多名人员使用该分类体系对多智能体系统执行过程中的故障进行分类时，其得出的结论应基本一致，这意味着该分类体系必须对各类故障模式的含义提供清晰明确的定义。

为了系统地揭示失败模式且不受偏见影响，我们采用扎根理论（GT）方法（Glaser & Strauss, 1967），这是一种定性研究方法，其理论构建直接基于实证数据，而非预先验证假设。GT的归纳性质使得失败模式能够自然地浮现出来。我们通过理论抽样、开放式编码、持续比较分析、撰写备忘录以及理论构建等方法，迭代式地收集并分析多智能体系统（MAS）的执行痕迹，具体细节详见第3.1节。总共对150多个执行痕迹进行GT分析，每位具有代理系统经验的标注者平均需要超过20小时的纯标注时间。

在获取MAS执行痕迹并讨论初步发现后，我们通过汇总观察到的失败模式，构建了一个初步的分类体系。为进一步优化该分类体系，我们开展了标注者间一致性（inter-annotator agreement, IAA）研究，通过不断增删、合并、拆分或修改失败模式及分类的定义，直至达成共识。这一过程类似于一种学习过程，分类体系的迭代优化持续进行，直至稳定，其稳定性通过Cohen’s Kappa系数衡量的IAA进行评估。为此，我们进行了三轮IAA实验，总计耗时约10小时，仅用于解决标注差异问题，不包括标注本身所花费的时间。

此外，为实现失败模式的自动化识别，我们开发了一种基于大语言模型（LLM）的标注工具，并对其可靠性进行了验证。

3.1 数据收集与分析  

我们采用理论采样（Draucker 等，2007）的方法，以确保所识别的多智能体系统（MAS）具有多样性，并确定数据收集的任务范围（即 MAS 执行轨迹）。该方法指导我们根据系统的目标差异、组织结构、实现方法以及底层智能体人格等方面的差异来选择 MAS。对于每个 MAS，我们选取的任务旨在体现系统的预期能力，而非人为设计的挑战性场景。例如，若某个系统在特定基准测试或数据集上报告了性能表现，我们就直接从这些基准中选取任务。所分析的 MAS 涉及多种领域和应用场景，详见表1和附录B。在收集到 MAS 的执行轨迹后，我们采用开放式编码（Khandkar，2009）对所收集的轨迹进行分析，重点关注智能体与智能体之间以及智能体与环境之间的交互。开放式编码将定性数据分解为带有标签的片段，使标注者能够创建新代码，并通过撰写备忘录记录观察结果，从而实现标注者之间的迭代反思与协作。特别是，标注者识别出所遇到的故障模式，并系统地将新创建的代码与已有代码进行比较，这一过程在扎根理论（GT）中被称为“持续比较分析”。这一故障模式识别与开放式编码的迭代过程持续进行，直到达到理论饱和——即不再从新增数据中获得新的洞察。通过这一过程，标注者共标注了超过150条轨迹，涵盖5个 MAS 系统：HyperAgent、AppWorld、AG2、ChatDev 和 MetaGPT。为获得超过150条轨迹，我们使用了多种基准测试来构建数据集。具体而言，我们为 HyperAgent 使用了 SWE-Bench-Lite，为 AppWorld 使用了 Test-C，为 MetaGPT 和 ChatDev 使用了 ProgramDev，为 AG2 使用了 GSM-Plus，每个系统均获得了超过30条轨迹。需要注意的是，表1中其余的 MAS 系统——OpenManus 和 Magentic-One——在本次扎根理论研究及后续的标注者间一致性（IAA）研究中均未使用，它们被保留作为第3.3节中讨论的一般化实验。接下来，我们将相关的开放式编码结果进行归类，以揭示 MAST 初始版本中的细粒度故障模式。最后，我们将各类故障模式相互关联，构建出如图2所示的错误类别分类体系。该过程在图3中以点1和点2表示。在提出初始分类体系后，一个关键问题是：该分类体系的可靠性如何？我们又该如何评估？

3.2  annotator间一致性研究与迭代优化  

annotator间一致性研究主要旨在验证某一测试或评分标准的有效性，即当多位不同的标注者基于相同的评分标准对同一组测试案例进行标注时，应得出一致的结论。尽管我们最初是通过前文所述的理论抽样和开放式编码得出分类体系的，但仍需进一步验证该分类体系的非歧义性。

为了进行标注者间一致性（IAA）研究，我们在初步构建分类体系的基础上开展了三轮主要讨论。在第一轮中，我们依据前文所述的理论抽样方法，从我们所获得的150多个MAS trace中随机选取了5个不同的trace，并由三位标注者使用初始分类体系中的故障模式及其定义对这些trace进行标注。我们发现，第一轮标注中三位标注者之间的一致性非常弱，Cohen’s Kappa评分为0.24。接下来，这些标注者对分类体系进行优化和修订，这包括反复调整分类体系，直至就所有5个收集到的trace中每个故障模式是否存在达成共识。在迭代优化过程中，我们根据需要修改故障模式的定义，将某些故障模式进一步细分以获得更精细的分类，或将多个故障模式合并为新的故障模式，或增加新的故障模式，或删除某些不再适用的故障模式。

6、 结论
 
在本研究中，我们首次对基于大语言模型（LLM）的多智能体系统（MAS）的故障模式进行了系统性调查。我们利用扎根理论分析了超过200条执行轨迹，并通过注释者间一致性研究，迭代地完善和验证了我们的分类体系。我们识别出14种细粒度的故障模式，并将其归纳为3个不同的类别，形成了多智能体系统故障分类体系（MAST）。MAST为未来的MAS研究提供了基础性框架。此外，我们开发并验证了一个自动化的评估流水线——LLM标注器，用于基于MAST实现可扩展的故障分析。该自动化标注工具为开发者提供了一种实用手段，能够系统化地诊断与评估问题，从而指导构建更加稳健的系统。  

我们对多智能体系统的潜力充满期待，但要实现其广泛应用，这些系统必须具备可靠性。MAST通过提供理解并缓解故障的框架，有助于实现这一目标。通过明确定义这些挑战，我们也为研究社区开放了具体的研究问题，鼓励大家共同协作解决。
