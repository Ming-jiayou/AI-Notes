REACT：在语言模型中协同推理与行动  
姚顺宇 ∗ *,1，赵杰弗里 2，于丹 2，杜楠 2，伊察克·夏弗兰 2，卡蒂克·纳拉西姆汉 1，曹原 2  
1 普林斯顿大学计算机科学系  
2 谷歌研究，Brain团队  
1 {shunyuy,karthikn}@princeton.edu  
2 {jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com

摘要  

尽管大型语言模型（LLMs）在语言理解与交互式决策任务中表现出色，但其推理能力（如思维链提示）与行动能力（如动作规划生成）的研究长期以来被视作独立的议题。本文探讨了一种将LLM用于以交错方式生成推理过程与特定任务动作的新方法，从而实现两者之间更强的协同效应：推理过程帮助模型推导、跟踪并更新行动方案，同时应对异常情况；而具体行动则使模型能够与外部资源（如知识库或环境）交互，并获取额外信息。我们将该方法命名为ReAct，并将其应用于一系列多样化的语言理解与决策任务，实验结果表明，相比现有最优基线方法，ReAct不仅显著提升了性能，还增强了人类对模型行为的可解释性与可信度。具体而言，在问答任务（HotpotQA）与事实验证任务（Fever）中，ReAct通过与简单的维基百科API交互，有效缓解了传统思维链推理中常见的幻觉与错误传播问题，生成了更接近人类思维方式的任务解决轨迹，且比不使用推理轨迹的基线方法更具可解释性。此外，在两个交互式决策基准任务（ALFWorld与WebShop）上，ReAct分别以绝对成功率超越模仿学习与强化学习方法34%和10%，且仅需一到两个上下文中的示例进行提示。

1 引言  

人类智能的一个独特特征，是能够无缝地将任务导向的行为与语言思维（或内在言语，Alderson-Day & Fernyhough，2015）结合起来。这种能力被认为在人类认知中起着重要作用，有助于自我调控或策略制定（Vygotsky，1987；Luria，1965；Fernyhough，2010），并维持工作记忆（Baddeley，1992）。以在厨房里烹饪一道菜肴为例：在任意两个具体动作之间，我们可能会用语言进行思考，以追踪进展（“现在所有食材都已经切好了，我该把水烧开了”）；处理意外情况或根据实际情况调整计划（“我没有盐，那改用酱油和胡椒吧”）；或者意识到需要外部信息时（“我该怎样制作面团？让我上网查一下”）。我们还可以采取行动来支持思考并回答问题（“我现在能做哪道菜？”）——例如打开一本菜谱查阅食谱、打开冰箱、检查手中有哪些食材。这种“行动”与“思考”之间紧密的协同关系，使得人类能够快速学习新任务，并在面对前所未见的情况或信息不确定时，依然能够进行稳健的决策或推理。

最近的研究表明，有望在自主系统中结合语言推理与交互式决策。一方面，经过适当提示的大型语言模型（LLMs）已展现出能够执行多步推理过程以得出答案的涌现能力，涉及算术、常识以及符号推理等任务（Wei et al., 2022）。然而，这种“思维链”推理是一种静态的黑箱，因为模型依赖其内部表征来生成思维，而并未与外部世界建立联系，这限制了其进行反应式推理或更新知识的能力。这可能导致诸如事实幻觉和推理过程中错误传播等问题（图1 (1b)）。另一方面，近期的研究探索了利用预训练语言模型在交互环境中进行规划与行动（Ahn 等，2022；Nakano 等，2021；Yao 等，2020；Huang 等，2022a），重点在于通过语言先验来预测行动。这些方法通常将多模态观测转换为文本，利用语言模型生成特定领域的动作或计划，然后再通过控制器选择或执行这些动作。然而，它们并未充分利用语言模型来对高层次目标进行抽象推理，也未建立工作记忆以支持持续决策，除Huang等人（2022b）之外，他们仅进行了有限的言语推理，仅重复描述当前状态的空间事实。除了这类简单的具身任务（如与几个积木互动）之外，目前尚无研究探讨如何协同结合推理与行动以实现通用任务求解，也缺乏对这种结合是否能带来系统性优势的深入分析——相较于单独使用推理或行动而言。

在本工作中，我们提出了ReAct，这是一种通用范式，通过将推理与行动相结合，应用于语言模型，以解决多种语言推理与决策任务（见图1）。ReAct通过交错式地引导大语言模型生成语言推理过程和与任务相关的具体行动，使模型能够进行动态推理，从而制定、维护和调整高层次的行动计划（即“为行动而推理”），同时还能与外部环境（如维基百科）交互，将额外信息融入推理过程（即“为推理而行动”）。

我们在四个多样化的基准数据集上对 ReAct 及其最先进的基线方法进行了实证评估：  
问答任务（HotPotQA，Yang 等，2018）、事实验证（Fever，Thorne 等，2018）、基于文本的游戏（ALFWorld，Shridhar 等，2020b），以及网页导航任务（WebShop，Yao 等，2022）。在 HotPotQA 和 Fever 任务中，通过访问模型可交互的 Wikipedia API，ReAct 在性能上优于传统的动作生成模型，同时与思维链推理（Chain-of-Thought, CoT）方法相当（Wei 等，2022）。总体表现最佳的方案是 ReAct 与 CoT 的结合，这种组合允许模型在推理过程中同时利用内部知识和外部获取的信息。在 ALFWorld 和 WebShop 任务中，仅需两步甚至一次提示的 ReAct 方法，就能超越使用 10³ ∼ 10⁵ 个任务实例训练的模仿学习或强化学习方法，其成功率分别提升了 34% 和 10%。我们还通过与仅依赖动作的受控基线方法对比，展示了稀疏且多功能推理在决策过程中的重要性，结果表现出持续的优势。除了具有广泛适用性与显著的性能提升外，推理与行动相结合的方式还显著增强了模型在各个领域的可解释性、可信度和可诊断性。人类能够轻松地区分模型基于内部知识的信息与来自外部环境的信息，同时可以审查推理过程，深入理解模型行为背后的决策依据。

综上所述，我们的主要贡献如下：（1）我们提出了ReAct，这是一种新颖的基于提示的范式，旨在将语言模型中的推理与行动相结合，以实现通用任务求解；（2）我们在多种基准测试上进行了广泛的实验，展示了在少样本学习设置下，ReAct相较于以往仅单独进行推理或行动生成的方法所具有的显著优势；（3）我们进行了系统的消融实验与分析，深入理解了行动在推理任务中的重要性，以及推理在交互式任务中的关键作用；（4）我们分析了在当前提示设置下ReAct的局限性（即推理和行动行为的支持有限），并开展了初步的微调实验，表明ReAct在获得更多训练数据后具有进一步提升的潜力。未来，通过将ReAct扩展到更多任务的训练与应用，并结合强化学习等互补范式，将进一步释放大型语言模型的潜能。


2 反应：协同推理与行动

考虑一个智能体与环境交互以完成任务的一般框架。在时间步 $ t $，智能体从环境中接收观测 $ o_t \in \mathcal{O} $，并根据某种策略 $ \pi(a_t | c_t) $ 采取动作 $ a_t \in \mathcal{A} $，其中上下文 $ c_t = (o_1, a_1, \cdots, o_{t-1}, a_{t-1}, o_t) $ 是智能体当前所依据的上下文信息。

当映射 $ c_t \mapsto a_t $ 极其隐式且需要大量计算时，学习一个有效策略将变得极具挑战性。例如，图1(1c)中的智能体无法生成正确的最终动作（动作4）来完成问答任务，因为它需要对整个轨迹上下文（问题、动作1-3、观测1-3）进行复杂的推理。类似地，图1(2a)中的智能体未能从上下文中理解“sinkbasin 1”中不包含“peppershaker 1”，因而持续生成幻觉类的动作。

ReAct的核心思想很简单：我们扩展智能体的动作空间为 $ \hat{\mathcal{A}} = \mathcal{A} \cup \mathcal{L} $，其中 $ \mathcal{L} $ 是语言空间。语言空间中的动作 $ \hat{a}_t \in \mathcal{L} $ 不影响外部环境，因此不会带来观测反馈。相反，这类动作（我们称之为“思考”或“推理痕迹”）旨在通过对当前上下文 $ c_t $ 的推理，来整合有用信息，并更新上下文为 $ c_{t+1} = (c_t, \hat{a}_t) $，以支持后续的推理或执行动作。如图1所示，有用的“思考”可以有多种类型：分解任务目标并制定行动方案（图2b，动作1；图1d，思考1）、注入与任务相关的常识知识（图2b，动作1）、从观测中提取关键信息（图1d，思考2、4）、跟踪任务进展并调整行动计划（图2b，动作8）、处理异常情况并动态修正计划（图1d，思考3）等。

然而，语言空间 $ \mathcal{L} $ 的大小是无限的，因此在这个扩展后的动作空间中学习非常困难，需要强大的语言先验知识。本文主要聚焦于一种设定：使用一个固定的大型语言模型 PaLM-540B（Chowdhery et al., 2022）1，通过少量上下文示例（few-shot in-context examples）来生成领域特定动作与自由形式语言“思考”以解决任务（见图1(1d)和(2b)）。每个上下文示例是一段人类解决任务实例的动作、思考与环境观测轨迹（详见附录C）。对于以推理为核心的任务（图1(1)），我们交替生成“思考”和“动作”，使得整个任务解决轨迹包含多个“思考-动作-观测”循环步骤。而对于可能涉及大量动作的决策任务（图1(2)），则只需在轨迹中最重要的位置稀疏地出现“思考”，我们让语言模型自行决定“思考”与“动作”的异步出现时机。

由于决策与推理能力被整合进一个大型语言模型中，ReAct具备以下几个独特优势：

A) 直观且易于设计：设计ReAct提示（prompt）非常直接——人类标注者只需在其执行的动作之上，用自然语言写下自己的思考过程即可。本文避免使用临时格式、特殊思考设计或示例选择。我们在第3节与第4节中详细说明了各类任务的提示设计。

B) 通用且灵活：得益于灵活的“思考”空间以及“思考-动作”出现方式的可变性，ReAct适用于各种具有不同动作空间和推理需求的任务，包括但不限于：问答（QA）、事实验证、文本游戏、网页导航等。

C) 高效且鲁棒：ReAct在仅使用1至6个上下文示例的情况下，展现了强大的泛化能力，持续优于仅依赖推理或仅依赖动作的基线模型，在多个领域表现卓越。我们在第3节进一步展示了启用微调（fine-tuning）后的额外优势，并在第4节说明了ReAct对提示选择的鲁棒性。

D) 与人类对齐且可控：ReAct提供了一个可解释的、顺序化的决策与推理过程，人类可以轻松审查其中的推理逻辑和事实准确性。此外，人类还能通过修改“思考”来实时干预或修正智能体行为，如图5所示（第4节）。

3 知识密集型推理任务

我们首先从知识密集型推理任务入手，例如多跳问答和事实验证。如图1(1d)所示，通过与维基百科API交互，ReAct能够检索支持推理的信息，同时利用推理来决定下一步需要获取什么内容，从而展现了推理与行动之间的协同作用。

3.1 设置  

我们考虑两个对知识检索和推理具有挑战性的数据集：(1) HotpotQA（Yang 等，2018），这是一个多跳问答基准数据集，要求模型在两个或更多维基百科段落之间进行推理；(2) FEVER（Thorne 等，2018），这是一个事实验证基准数据集，每个陈述（claim）会被标注为 SUPPORTS（支持）、REFUTES（反驳）或 NOT ENOUGH INFO（信息不足），具体取决于是否存在维基百科段落能够验证该陈述。在本研究中，我们采用“仅问题”（question-only）的设置，即模型仅接收问题或陈述作为输入，无法访问支持段落，必须依赖自身内部知识，或通过与外部环境交互来检索知识以支持推理。

动作空间 我们设计了一个简单的维基百科网页API，包含三种动作类型，以支持交互式信息检索：(1) 搜索 [实体]，如果该实体对应的维基百科页面存在，则返回该页面的前5句话；否则，会从维基百科搜索引擎中推荐5个最相似的实体；(2) 查找 [字符串]，返回页面中包含该字符串的下一句内容，模拟浏览器中的Ctrl+F查找功能；(3) 完成 [答案]，表示以指定答案结束当前任务。我们注意到，这种动作空间主要只能基于精确的页面名称检索文本片段的很小一部分，其能力远弱于当前最先进的词汇或神经检索器。设计的目的是模拟人类与维基百科的交互方式，并迫使模型通过语言表达的显式推理来完成信息检索。

基线设置 我们系统性地对 ReAct 轨迹进行消融实验，以构建多个基线（格式如图1 (1a-1c) 所示）：  
(a) 标准提示（Standard）：移除 ReAct 轨迹中的所有思考、动作和观察。  
(b) 思维链提示（CoT）(Wei et al., 2022)，移除动作和观察，作为仅依赖推理的基线。我们还通过在推理过程中使用采样温度 0.7 生成 21 条 CoT 轨迹，采用多数票决定答案，构建了一个自一致性基线（CoT-SC）(Wang et al., 2022a; b)，该方法被发现 consistently 能够提升 CoT 的性能。  
(c) 仅执行提示（Act）：移除 ReAct 轨迹中的思考部分，其行为大致类似于 WebGPT (Nakano et al., 2021) 与互联网交互以回答问题的方式，尽管其任务和动作空间不同，并采用模仿学习与强化学习而非提示方式。

融合内部与外部知识 如第3.3节将详细说明，我们观察到 ReAct 所展示的问题解决过程更具事实性且立足于现实，而 CoT 在构建推理结构方面更为准确，但容易出现幻觉性的事实或思考。因此，我们提出结合 ReAct 与 CoT-SC，并让模型根据以下启发式规则决定何时切换到另一种方法：  
A) ReAct → CoT-SC：当 ReAct 在给定步数内未能返回答案时，退回到 CoT-SC。我们为 HotpotQA 和 FEVER 分别设定 7 步和 5 步，因为发现增加步数并不会进一步提升 ReAct 的性能。  
B) CoT-SC → ReAct：当 n 个 CoT-SC 样本中的多数答案出现次数少于 n/2（即内部知识可能无法自信地支持该任务），则退回到 ReAct。

微调 由于在大规模上人工标注推理轨迹和动作具有挑战性，我们采用类似于 Zelikman 等人（2022）的自举方法，使用由 ReAct（也适用于其他基线）生成的 3,000 条正确答案轨迹，对较小的语言模型（PaLM-8/62B）进行微调，使其能够根据输入的问题/断言生成完整的轨迹（所有思考、动作、观察）。更多细节见附录 B.1。

3.3 结果与观察  
ReAct在所有情况下均优于Act。表1展示了以PaLM-540B作为基础模型，采用不同提示方法在HotpotQA和Fever两个任务上的实验结果。我们注意到，ReAct在两个任务上均优于Act，这表明推理过程对指导行动具有重要价值，特别是在整合得出最终答案时，如图1（1c-d）所示。微调结果3也进一步证实了推理轨迹能够使行动决策更加明智。

另一方面，ReAct在Fever数据集上的表现优于CoT（60.9对比56.3），但在HotpotQA上略逊于CoT（27.4对比29.4）。Fever任务中“SUPPORTS”与“REFUTES”类别的判断差异通常很小（详见附录D.1），因此通过行动获取准确且最新的知识显得尤为重要。为了更深入理解ReAct与CoT在HotpotQA上的行为差异，我们分别从ReAct和CoT中随机抽取了50条推理轨迹（共100条），这些轨迹对应于正确和错误的答案（以EM指标判断），并手动标注了它们的成功与失败模式，结果见表2。一些关键观察如下：

A）对CoT而言，幻觉是一个严重问题，在成功模式下，其误报率远高于ReAct（14% vs. 6%），且成为其主要的失败原因（占比56%）。相比之下，ReAct的问题求解过程更加务实、基于事实且更值得信赖，这得益于其能够访问外部知识库。

B）尽管交错进行推理、动作和观察步骤能够提升ReAct的环境贴合性与可信度，但这种结构上的约束也降低了其在制定推理步骤时的灵活性，导致其推理错误率高于CoT。我们注意到，ReAct存在一种典型的错误模式：模型会反复生成之前的思维和动作，我们将其归类为“推理错误”之一，因为模型未能正确推理出下一步应采取的适当行动，从而陷入循环无法跳出。

C）对于ReAct而言，通过搜索成功获取有用信息至关重要。非信息性搜索占错误案例的23%，这会打断模型的推理过程，使其难以恢复并重新组织思路。这或许是在事实性与灵活性之间不可避免的一种权衡，也正因此激发了我们提出结合两种方法的策略。

我们在附录E.1中为每种成功与失败模式提供了示例。此外，我们发现部分HotpotQA问题可能包含过时的答案标签，具体示例如图4所示。 

在提示大型语言模型方面，ReAct + CoT-SC表现最佳。如表1所示，在HotpotQA和Fever数据集上表现最佳的提示方法分别为ReAct → CoT-SC和CoT-SC → ReAct。进一步地，图2展示了不同方法在使用不同数量的CoT-SC样本时的表现差异。虽然两种ReAct + CoT-SC方法分别在一项任务上占优，但它们在不同样本数量下均显著且一致地优于CoT-SC，在仅使用3到5个样本的情况下，便可达到使用21个样本时CoT-SC的性能水平。这些结果表明，在推理任务中，合理结合模型内部知识与外部知识具有重要价值。

ReAct在微调时表现最佳。图3展示了在HotpotQA数据集上，四种方法（Standard、CoT、Act、ReAct）在提示（prompting）和微调（finetuning）下的缩放效应。使用PaLM-8/62B模型时，仅通过提示方式使用ReAct的表现最差，原因是模型难以从上下文样例中同时学习推理和行动能力。然而，当仅用3,000个样本进行微调后，ReAct反而成为四种方法中表现最佳的模型：PaLM-8B微调后的ReAct超过了所有PaLM-62B的提示方法，而PaLM-62B微调后的ReAct甚至优于所有540B参数的提示方法。

相比之下，对Standard或CoT方法进行微调的效果显著逊于ReAct或Act方法，无论是在PaLM-8B还是PaLM-62B模型上均如此。原因在于，前者的微调本质上促使模型记忆（可能包含幻觉的）知识事实，而后者则教会模型如何进行推理并执行操作，以从维基百科中获取信息——这是一种更具泛化能力的知识推理技能。

尽管所有提示方法目前仍与特定领域最先进的方法存在显著差距（见表1），我们认为，通过更多人工编写的高质量数据进行微调，可能是充分挖掘ReAct潜力的更好途径。

4 决策任务  

我们还在两个基于语言的交互式决策任务——ALFWorld 和 WebShop 上测试了 ReAct。这两个任务均具有复杂的环境，要求智能体在稀疏奖励的条件下进行长时间的决策与行动，因此需要有效的推理来指导行动和探索。

ALFWorld（Shridhar等，2020b）（图1(2)）是一个合成的基于文本的游戏环境，旨在与具身化的ALFRED基准任务（Shridhar等，2020a）对齐。该环境包含6类任务，其中智能体需要通过文本动作（如“前往咖啡桌1”“拿起纸张2”“使用书桌灯1”）在模拟的家用环境中导航并执行操作，以达成高层级目标（例如：检查书桌灯下的纸张）。一个任务实例可能包含超过50个位置，且需要专家策略超过50步才能完成，因此对智能体提出了较高的挑战，要求其具备规划能力、子目标跟踪能力以及系统化的探索能力（例如：逐一检查每张桌子以寻找书桌灯）。特别地，ALFWorld中内置了一个挑战：智能体需要判断常见家用物品可能出现的合理位置（例如，书桌灯很可能出现在书桌、搁架或衣柜上），这使得该环境非常适合利用大型语言模型（LLM）所具备的预训练常识知识。

为了引导ReAct（Reasoning and Acting）框架，我们从训练集中随机标注每类任务的三条轨迹，每条轨迹包含稀疏的思维过程，这些思维过程分别实现以下功能：(1) 将整体目标分解为子目标，(2) 跟踪子目标的完成情况，(3) 确定下一步的子目标，(4) 通过常识推理判断物品的可能位置以及应采取的操作。我们在附录C.4中展示了用于ALFWorld的完整提示模板。依照Shridhar等人（2020b）的做法，我们在特定任务设置下评估了134个未见过的测试游戏任务。为保证实验的稳健性，我们通过从所标注的3条轨迹中选取2条的全部排列组合，为每类任务构建了6种不同的提示。Act提示则使用相同的轨迹数据，但不包含思维过程——由于任务实例是从训练集中随机选择的，因此这种方式不会倾向于ReAct或Act，从而提供了一个公平、可控的对比实验，用以检验稀疏思维对性能的影响。

作为基线方法，我们采用BUTLER（Shridhar等，2020b），这是一个基于模仿学习训练的智能体，其训练数据为每类任务类型10^5条专家轨迹。

WebShop Can ReAct 也能在嘈杂的现实语言环境中进行交互，以适用于实际应用吗？我们研究了 WebShop（Yao 等，2022），这是一个最近提出的在线购物网站环境，包含 118 万种真实世界的产品和 1.2 万个由人类提供的指令。与 ALFWorld 不同，WebShop 包含大量结构化和非结构化文本（例如从亚马逊抓取的产品标题、描述和选项），要求智能体通过网页交互（如搜索“nightstand drawers”、选择“color: modern-nickel-white”或“back to search”等按钮）根据用户指令购买特定产品（例如：“我在找带抽屉的床头柜，表面为镍色，价格低于 140 美元”）。该任务通过平均得分（所有回合中所选产品覆盖期望属性的百分比）和成功率（所选产品满足所有要求的回合所占百分比）在 500 条测试指令上进行评估。

我们设计了 Act 提示，包含“搜索”、“选择商品”、“选择选项”和“购买”等动作；而 ReAct 提示则在基础上增加了推理能力，用于判断应探索什么内容、何时购买，以及哪些商品选项与指令相关。详见表 6 的示例提示，以及附录中的表 10 的模型预测结果。我们与一种模仿学习（IL）方法进行比较，Micheli & Fleuret（2021）在 3,553 个任务实例上微调了一个 GPT-2 模型，性能远超 BUTLER，但由于该方法在所有任务类型上进行了训练，因此未被列入基准对比。我们还比较了基于 1,012 条人类标注轨迹训练的模仿学习方法，以及进一步使用 10,587 条训练指令进行模仿+强化学习（IL + RL）训练的方法。

结果表明，ReAct在ALFWorld（表3）和Webshop（表4）两项任务上均优于Act。在ALFWorld任务中，表现最佳的ReAct实验达到了71%的平均成功率，显著超过表现最佳的Act（45%）和BUTLER（37%）实验。事实上，即使ReAct的最差实验（48%）也优于两种方法的最佳表现。此外，在六次受控实验中，ReAct相对于Act的优势始终存在，相对性能提升在33%至90%之间，平均达到62%。从定性分析来看，我们发现Act完全缺乏思考过程，无法正确将目标任务分解为更小的子目标，或在环境状态变化时丢失上下文信息。ReAct与Act的典型任务轨迹对比可参见附录D.2.1和D.2.2。

在Webshop任务中，仅通过一次提示（one-shot prompting）的Act方法表现已与IL及IL+RL方法相当。而在引入稀疏推理后，ReAct的性能显著提升，比此前最佳成功率提高了10%的绝对值。通过分析具体示例发现，ReAct更善于通过推理弥合嘈杂观察与实际操作之间的差距，从而识别出与指令相关的产品与选项。例如：“对于‘客厅用节省空间的沙发凳’，该商品有‘39x18x18英寸’和‘蓝色’等选项，看起来值得购买。”然而，现有方法与专家人类的表现仍有显著差距（见表4），人类专家能够进行更频繁的产品探索和查询重构，这些行为目前对基于提示的方法而言仍具有挑战性。

关于内部推理与外部反馈的价值：据我们所知，ReAct 是首个在闭环系统中，将大型语言模型（LLM）应用于交互式环境、并结合推理与行动的演示。最接近的先前工作是 Huang 等人（2022b）提出的“内心独白”（Inner Monologue, IM），其中具身智能体的行为由所谓的“内心独白”驱动。然而，IM 的“内心独白”仅限于对环境状态的观察以及为实现目标智能体需要完成的任务。相比之下，ReAct 中用于决策的推理痕迹具有灵活性和稀疏性，能够针对不同任务诱导出多样化的推理类型（参见第2节）。

为了展示ReAct与IM之间的差异，并突出内部推理相较于对外部反馈的简单反应的重要性，我们进行了一项消融实验，使用了一种类似于IM的、由密集外部反馈构成的思维模式。如表3所示，ReAct在整体成功率上显著优于IM风格的提示方法（ReAct-IM）（71% 对 53%），并在六个任务中的五个任务上均保持稳定优势。定性观察发现，ReAct-IM常常在识别子目标是否完成、或下一步该执行哪个子目标时出现错误，原因在于缺乏高层次目标分解能力。此外，许多ReAct-IM的执行轨迹难以判断物品在ALFWorld环境中可能的位置，这是因为缺乏常识性推理。上述两种不足均可通过ReAct范式加以解决。关于ReAct-IM的更多细节见附录B.2；ReAct-IM的示例提示见附录C.4；示例轨迹见附录D.2.3。

5 相关工作

基于语言模型的推理：或许最广为人知的利用大语言模型（LLM）进行推理的工作是“思维链”（Chain-of-Thought, CoT）（Wei 等，2022），该方法揭示了 LLM 能够自主构建解决问题的“思维过程”。此后，多项后续研究相继展开，包括用于解决复杂任务的“由少至多”提示（least-to-most prompting）（Zhou 等，2022）、零样本 CoT（zero-shot-CoT）（Kojima 等，2022），以及基于自一致性进行推理（Wang 等，2022a）。最近，Madaan 与 Yazdanbakhsh（2022）系统地研究了 CoT 的表达形式与结构，发现符号、模式和文本的存在对 CoT 的有效性至关重要。其他研究也已扩展至超越简单提示的更为复杂的推理架构。例如，Selection-Inference（Creswell 等，2022）将推理过程划分为“选择”和“推理”两个步骤；STaR（Zelikman 等，2022）通过使用模型自身生成的正确推理依据对模型进行微调，来引导推理过程；Faithful reasoning（Creswell & Shanahan，2022）将多步推理分解为三个步骤，分别由三个独立的语言模型完成。类似的方法如 Scratchpad（Nye 等，2021），通过对模型在中间计算步骤上的表现进行微调，也在多步计算问题上展现出显著提升。

与这些方法不同，ReAct 不仅执行孤立的、固定的推理，还将模型的动作与其对应的观测结果整合为连贯的输入流，从而帮助模型更精准地推理，并应对超越纯推理的任务（例如，交互式决策）。

用于决策的语言模型：大型语言模型（LLMs）强大的能力使其能够执行超越语言生成的任务，近年来，越来越多地将LLMs用作决策的策略模型，尤其在交互式环境中。WebGPT（Nakano等，2021）利用语言模型与网页浏览器交互、浏览网页，并从ELI5数据集（Fan等，2019）中推断复杂问题的答案。与ReAct不同，WebGPT并未显式建模思维与推理过程，而是依赖昂贵的人类反馈进行强化学习。在对话建模领域，诸如BlenderBot（Shuster等，2022b）和Sparrow（Glaese等，2022）等聊天机器人，以及SimpleTOD（Hosseini-Asl等，2020）等任务导向型对话系统，也训练语言模型以决定是否调用API。然而，与ReAct不同，它们同样没有显式考虑推理过程，也依赖昂贵的数据集和人工反馈采集来学习策略。相比之下，ReAct以一种低成本的方式学习策略，因为其决策过程仅需对推理步骤进行语言描述即可。

大型语言模型（LLMs）也越来越被用于交互式和具身环境中的规划与决策。在这方面，与ReAct最为相关的是SayCan（Ahn等，2022年）和Inner Monologue（Huang等，2022b），二者均利用LLM进行机器人动作规划与决策。在SayCan中，LLM被提示直接预测机器人可能采取的动作，随后这些预测由基于视觉环境的可用性模型重新排序，以生成最终决策。Inner Monologue在此基础上进一步改进，引入了名为“内心独白”的机制，该机制通过环境反馈的形式注入到系统中。据我们所知，Inner Monologue是首个展示此类闭环系统的开创性工作，而ReAct正是在此基础上发展的。然而，我们认为Inner Monologue并未真正包含“内心思考”——这一点将在第4节详细阐述。此外，我们也注意到，在其他交互式决策场景中，利用语言作为语义丰富的输入已证明是成功的（Abramson等，2020；Karamcheti等，2021；Huang等，2022a；Li等，2022）。随着LLM的进步，语言作为一种基础性的认知机制，在交互与决策中的关键作用正变得日益明显。不仅如此，LLM的进展也激发了像Reed等（2022）所提出的多功能、通用型智能体的发展。

6 结论

我们提出了ReAct——一种简单而有效的机制，用于在大语言模型中协同推理与行动。通过在多跳问答、事实核查和互动决策任务上的一系列多样化实验，我们表明ReAct能够实现更优的性能，并提供可解释的决策轨迹。尽管我们的方法简单，但在具有大规模动作空间的复杂任务中，模型需要更多的示例才能良好学习，而这不幸地很容易超出上下文学习的输入长度限制。我们探索了在HotpotQA上进行微调的方法，并取得了初步的积极结果，但进一步提升性能仍需依赖更多高质量的人类标注数据。通过多任务训练扩展ReAct，并将其与强化学习等互补范式结合，有望催生更强大的智能体，从而进一步释放大语言模型在更多应用场景中的潜力。